{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrGStUsXmJZfoTR8cePEu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mragankk/Object_detection_and_Depth_estimation/blob/main/YOLO%2BDepth_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Object Detection & Depth Estimation using YOLO and Apple's ML-Depth-Pro\n",
        "\n",
        "This notebook performs:\n",
        "- **Object detection** using [Ultralytics YOLO](https://github.com/ultralytics/ultralytics)\n",
        "- **Depth estimation** using Apple's [ML-Depth-Pro](https://github.com/apple/ml-depth-pro)\n",
        "- Visualization of results with bounding boxes and depth values\n"
      ],
      "metadata": {
        "id": "Rp4QPxISnzt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**\n",
        "- **YOLO** (Ultralytics) for object detection\n",
        "- **ML-Depth-Pro** for depth estimation\n",
        "- OpenCV, PyTorch, Pillow, Open3D for image processing and 3D visualization\n"
      ],
      "metadata": {
        "id": "2TLhRtFPoBIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3PTK-NgCWrQ",
        "outputId": "62f0fcad-4438-44d0-8a0a-d4fd3d9242ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/188.7 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install numpy opencv-python==4.9.0.80 opencv-contrib-python==4.9.0.80 opencv-python-headless==4.9.0.80 -q\n",
        "\n",
        "%pip install ultralytics timm torch torchvision open3d pillow -q\n",
        "\n",
        "!rm -rf ml-depth-pro\n",
        "!git clone https://github.com/apple/ml-depth-pro -q\n",
        "%cd ml-depth-pro\n",
        "!pip install -e . -q\n",
        "%cd ..\n",
        "\n",
        "!mkdir -p checkpoints\n",
        "\n",
        "!hf download apple/DepthPro depth_pro.pt --local-dir checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restart the Runtime**\n",
        "\n",
        "After the installation completes successfully, restart the Colab runtime to ensure all newly installed packages are loaded properly.  \n",
        "\n",
        "You can do this by:  \n",
        "- Clicking on **Runtime** in the top menu  \n",
        "- Selecting **Restart session**  \n",
        "- Then re-running the next cells"
      ],
      "metadata": {
        "id": "yAKzrTxmtSr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Upload Images**\\\n",
        "Upload your test image from your local machine."
      ],
      "metadata": {
        "id": "VHSBDMEoZraX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Uploaded file: {filename}\")"
      ],
      "metadata": {
        "id": "9puqp3arXhXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Detection and Depth Estimation\n",
        "We use YOLO to detect objects in the image and store their bounding boxes & labels."
      ],
      "metadata": {
        "id": "3NOI_UUwVNJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import depth_pro as dp\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load YOLO model\n",
        "yolo_model = YOLO('/content/yolo11x.pt', verbose=False).to(device)\n",
        "if device == 'cuda':\n",
        "    yolo_model = yolo_model.cuda()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "image_path = '/content/lab.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "original_height, original_width = image.shape[:2]\n",
        "\n",
        "results = yolo_model(image)\n",
        "\n",
        "# Store bounding boxes and class names\n",
        "obj_boxes = []\n",
        "obj_names = []\n",
        "for r in results:\n",
        "    boxes = r.boxes.xyxy.cpu().numpy()\n",
        "    classes = r.boxes.cls.cpu().numpy()\n",
        "    for box, cls in zip(boxes, classes):\n",
        "        x1, y1, x2, y2 = map(int, box[:4])\n",
        "        obj_boxes.append((x1, y1, x2, y2))\n",
        "        obj_names.append(r.names[int(cls)])\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "pil_image = Image.open(image_path).convert(\"RGB\")"
      ],
      "metadata": {
        "id": "p0Tsht4zPnhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Depth Estimation\n",
        "We pass the image into Apple's ML-Depth-Pro model to estimate the distance (in meters) for each detected object.\n"
      ],
      "metadata": {
        "id": "yQ97w8VqqTEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depth_model, transform = dp.create_model_and_transforms()\n",
        "depth_model = depth_model.to(device).eval()\n",
        "\n",
        "img, _, f_px = dp.load_rgb(image_path)\n",
        "depth_input = transform(img).to(device)\n",
        "\n",
        "prediction = depth_model.infer(depth_input, f_px=f_px)\n",
        "depth = prediction[\"depth\"]  # depth in 'm'\n",
        "\n",
        "depth_np = depth.squeeze().cpu().numpy()\n",
        "\n",
        "for (x1, y1, x2, y2), obj_name in zip(obj_boxes, obj_names):\n",
        "    center_x = (x1 + x2) // 2\n",
        "    center_y = (y1 + y2) // 2\n",
        "\n",
        "    depth_value = depth_np[center_y, center_x]\n",
        "    text = f'{obj_name}: {depth_value:.2f}m'\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 1.2\n",
        "    font_thickness = 2\n",
        "    text_size = cv2.getTextSize(text, font, font_scale, font_thickness)[0]\n",
        "\n",
        "    text_x = x1\n",
        "    text_y = y1 - 10\n",
        "    rect_x1 = text_x - 5\n",
        "    rect_y1 = text_y - text_size[1] - 10\n",
        "    rect_x2 = text_x + text_size[0] + 5\n",
        "    rect_y2 = text_y + 5\n",
        "\n",
        "    cv2.rectangle(image, (rect_x1, rect_y1), (rect_x2, rect_y2), (0, 0, 0), -1)\n",
        "    cv2.putText(image, text, (text_x, text_y), font, font_scale, (255, 255, 255), font_thickness)\n",
        "\n",
        "resized_img = cv2.resize(image, (2500, 3000), interpolation=cv2.INTER_CUBIC)\n",
        "cv2.imwrite('/content/results/final_image.jpg', resized_img)\n",
        "cv2_imshow(resized_img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "G7l_bnQXqKpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Ss3_a0oCPwJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Object Detection"
      ],
      "metadata": {
        "id": "Hs7eXYH7X5Oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import depth_pro as dp\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "yolo_model = YOLO('/content/yolo11x.pt', verbose=False).to(device)\n",
        "if device == 'cuda':\n",
        "    yolo_model = yolo_model.cuda()\n",
        "\n",
        "img_path='/content/lab.jpg'\n",
        "image=cv2.imread(img_path)\n",
        "result=yolo_model(image)"
      ],
      "metadata": {
        "id": "BqkzMCT5L3_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bounding Boxes"
      ],
      "metadata": {
        "id": "RumrGrMaq-aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj_boxes=[]\n",
        "for r in result:\n",
        "    boxes=r.boxes.xyxy.cpu().numpy()\n",
        "    classes=r.boxes.cls.cpu().numpy()\n",
        "\n",
        "    for box,cls in zip(boxes,classes):\n",
        "        if r.names[int(cls)]=='chair': # check coco.names for other object classes\n",
        "            x1,y1,x2,y2=map(int,box[:4])\n",
        "            obj_boxes.append((x1,y1,x2,y2))\n",
        "            cv2.rectangle(image,(x1,y1),(x2,y2),(0,255,0),2)\n",
        "\n",
        "resized_img=cv2.resize(image,(500,500),interpolation=cv2.INTER_CUBIC)\n",
        "cv2_imshow(resized_img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "0Krzx7iyPEcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Convert depth tensor to NumPy array\n",
        "depth_np = depth.cpu().numpy().squeeze()\n",
        "\n",
        "# Save as an image (normalize and convert to 8-bit for visibility)\n",
        "depth_normalized = cv2.normalize(depth_np, None, 0, 255, cv2.NORM_MINMAX)\n",
        "depth_uint8 = np.uint8(depth_normalized)  # Convert to uint8\n",
        "\n",
        "# Save using OpenCV\n",
        "cv2.imwrite(\"depth_map_for_image.png\", depth_uint8)\n",
        "\n",
        "# Show with Matplotlib\n",
        "plt.imshow(depth_np, cmap='magma')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dNMrylK-PT4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "\n",
        "# Load the depth map\n",
        "depth_map = cv2.imread(\"/content/depth_map_for_image.png\", cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "# Check if the image is loaded\n",
        "if depth_map is None:\n",
        "    raise FileNotFoundError(\"Error: The depth image file was not loaded. Check the file path.\")\n",
        "\n",
        "# Invert the depth map (flip depth values)\n",
        "depth_map = cv2.bitwise_not(depth_map)  # Works for 8-bit depth\n",
        "# If depth is 16-bit or floating point, use: depth_map = np.max(depth_map) - depth_map\n",
        "\n",
        "# Normalize and apply a colormap for visualization\n",
        "depth_normalized = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
        "depth_colored = cv2.applyColorMap(depth_normalized.astype(np.uint8), cv2.COLORMAP_JET)\n",
        "\n",
        "# Show fixed depth map\n",
        "cv2_imshow(depth_colored)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Convert to Open3D Image\n",
        "depth_o3d = o3d.geometry.Image(depth_map.astype(np.uint16))\n",
        "\n",
        "# Camera intrinsics (modify based on camera)\n",
        "fx, fy, cx, cy = 525, 525, 319.5, 239.5\n",
        "intrinsic = o3d.camera.PinholeCameraIntrinsic(640, 480, fx, fy, cx, cy)\n",
        "\n",
        "# Create point cloud\n",
        "pcd = o3d.geometry.PointCloud.create_from_depth_image(depth_o3d, intrinsic)\n",
        "\n",
        "# Visualize\n",
        "o3d.visualization.draw_geometries([pcd])"
      ],
      "metadata": {
        "id": "QuKPEbE4dsPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run Depth Estimation**\n",
        "- Loads the uploaded image\n",
        "- Passes it through the DepthPro model\n",
        "- Generates a depth map\n",
        "- Saves and displays the result"
      ],
      "metadata": {
        "id": "obsl3HNrro31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# depth model and preprocessing transform\n",
        "\n",
        "# depth_model, transform = dp.create_model_and_transforms()\n",
        "# depth_model.eval()\n",
        "depth_model, transform = dp.create_model_and_transforms()\n",
        "depth_model = depth_model.to(device).eval()\n",
        "\n",
        "img,_,f_px=dp.load_rgb(img_path)\n",
        "depth_input=transform(img).to(device)\n",
        "\n",
        "prediction=depth_model.infer(depth_input, f_px=f_px)\n",
        "depth=prediction[\"depth\"] # depth in 'm'\n",
        "\n",
        "depth_np=depth.squeeze().cpu().numpy()\n",
        "for x1,y1,x2,y2 in obj_boxes:\n",
        "    center_x=(x1+x2)//2\n",
        "    center_y=(y1+y2)//2\n",
        "\n",
        "    depth_value=depth_np[center_y,center_x]\n",
        "    text=f'Depth: {depth_value:.2f}m'\n",
        "    font=cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale=1.2\n",
        "    font_thickness=2\n",
        "    text_size=cv2.getTextSize(text,font,font_scale,font_thickness)[0]\n",
        "\n",
        "    text_x=x1\n",
        "    text_y=y1-10\n",
        "    rect_x1=text_x-5\n",
        "    rect_y1=text_y-text_size[1]-10\n",
        "    rect_x2=text_x+text_size[0]+5\n",
        "    rect_y2=text_y+5\n",
        "\n",
        "    cv2.rectangle(image,(rect_x1,rect_y1),(rect_x2,rect_y2),(0,0,0),-1)\n",
        "    cv2.putText(image,text,(text_x,text_y),font,font_scale,(255,255,255),font_thickness)\n",
        "\n",
        "# image=cv2.resize(image,(1000,1000),interpolation=cv2.INTER_CUBIC)\n",
        "cv2_imshow(image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "cv2.imwrite(\"chair_detection_with_depth.jpg\",image)"
      ],
      "metadata": {
        "id": "o6M8XsNFmxrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}